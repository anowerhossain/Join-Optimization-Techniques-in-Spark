## Join Optimization Techniques in Spark  
Joins in Apache Spark are expensive operations because they involve data movement across the cluster. If not optimized, joins can cause:

âŒ Slow Performance â€“ Due to expensive shuffles across nodes.

âŒ High Memory Usage â€“ Large joins can cause OutOfMemory (OOM) errors.

âŒ Network Bottlenecks â€“ Large data transfers slow down execution.

Thus, join optimization is crucial to ensure efficient and scalable Spark performance.

## Join Optimizations in Spark

### 1ï¸âƒ£ Broadcast Join (Avoids Shuffle)

ğŸ‘‰ Best for: Small vs. Large Dataset
- Broadcasts the smaller dataset to all nodes
- Eliminates shuffling, making the join super fast ğŸš€

```python
# Using Broadcast Join
broadcast_df = large_df.join(broadcast(small_df), "country_code")
broadcast_df.explain(True)  # See how Spark optimizes the query

# Action to trigger execution
broadcast_df.count()
```
âœ… Fast execution (no shuffle)

âœ… Reduces memory & network overhead

